{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks with TensorFlow's Dataset API and Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TensorFlow 1.4, the Dataset API is [introduced](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html). The real advantage of the Dataset API is that a lot of memory management is done for the user when using large file-based datasets. And, in this work, we will be implementing a predefined DNN estimator and feed it with the Dataset API for Kaggle's Titanic dataset.\n",
    "\n",
    "---\n",
    "\n",
    "With Dataset API we can use file-based datasets or datasets in the memory. In this work we will read the data from a csv file. In order to have it, you should first run [this file](./01-data-label-encoding.ipynb) to label encode and then [this file](./02-data-feature-engineering.ipynb) for feature engineering and finally run [this file](split-train-valid.ipynb) to split the train set into train and valid sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset includes the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Title</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>FamilyID</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>247.5208</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>372</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.4958</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.5000</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>696</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.5000</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>867</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8583</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass  Sex   Age  SibSp  Parch      Fare  Title  Embarked  \\\n",
       "0          300       1    0  50.0      0      1  247.5208      7         1   \n",
       "1          372       3    1  18.0      1      0    6.4958      6         3   \n",
       "2          563       2    1  28.0      0      0   13.5000      6         3   \n",
       "3          696       2    1  52.0      0      0   13.5000      6         3   \n",
       "4          867       2    0  27.0      1      0   13.8583      4         1   \n",
       "\n",
       "   FamilySize  FamilyID  Survived  \n",
       "0           2        50         1  \n",
       "1           2        50         0  \n",
       "2           1        50         0  \n",
       "3           1        50         0  \n",
       "4           2        50         1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./data/train_split_final.csv')\n",
    "valid = pd.read_csv('./data/valid_split_final.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`{Pclass, Sex, Age, SibSp, Parch, Fare, Title, Embarked, FamilySize, FamilyID}` we will be using as the features and the `Survived` column will be our labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the Datasets API and feed the Estimator, we should write an input function like this:\n",
    "\n",
    "```python\n",
    "def input_fn():\n",
    "    ...<code>...\n",
    "    return ({ 'Pclass':[values], ..<etc>.., 'FamilyID':[values] },\n",
    "            [Survived])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes the `file_path` as input and outputs a two-element tuple. The first element of the tuple is a dictionary containing feature names as keys and features as values. And the second element is a list of labels for the training batch.\n",
    "\n",
    "Other two arguments for the input function are `perform_shuffle` and `repeat_count`. If `perform_shuffle` is `True` the order of the examples are shuffled. The `perform_shuffle` argument specifies the number of epochs during training, for instance, if `perform_shuffle=1` all the train set examples are passed only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the implementation is as follows, we will use this function to feed the estimator later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature names first\n",
    "\n",
    "feature_names = [\n",
    "    'Pclass',\n",
    "    'Sex',\n",
    "    'Age',\n",
    "    'SibSp',\n",
    "    'Parch',\n",
    "    'Fare',\n",
    "    'Title',\n",
    "    'Embarked',\n",
    "    'FamilySize'\n",
    "    'FamilyID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titanic_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
    "    def decode_csv(line):\n",
    "        # second argument of decode_csv defines the data types for each dataset column!\n",
    "        # the first argument is passenger ids thus integer\n",
    "        # the last column is survived or not labels thus integer\n",
    "        # and the rest are float.\n",
    "        parsed_line = tf.decode_csv(\n",
    "            line, [[0], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0]])\n",
    "        label = parsed_line[-1:] # Last element is the label\n",
    "        del parsed_line[-1] # Delete last element (it is the labels)\n",
    "        features = parsed_line[1:] # First element is excluded since it is the id column\n",
    "        d = dict(zip(feature_names, features)), label\n",
    "        return d\n",
    "    \n",
    "    dataset = (tf.data.TextLineDataset(file_path) # Read text file\n",
    "        .skip(1) # Skip header row\n",
    "        .map(decode_csv)) # Transform each elem by applying decode_csv fn\n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count) # Repeats dataset this # times\n",
    "    dataset = dataset.batch(32)  # Batch size to use\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory management is provided here with `TextLineDataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print and check the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train_final.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'Pclass': array([ 3.,  1.,  3.,  1.,  3.,  3.,  1.,  3.,  3.,  2.,  3.,  1.,  3.,\n",
      "        3.,  3.,  2.,  3.,  2.,  3.,  3.,  2.,  2.,  3.,  1.,  3.,  3.,\n",
      "        3.,  1.,  3.,  3.,  1.,  1.], dtype=float32), 'Sex': array([ 1.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
      "        1.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,\n",
      "        1.,  1.,  0.,  1.,  1.,  0.], dtype=float32), 'Age': array([ 22.,  38.,  26.,  35.,  35.,   0.,  54.,   2.,  27.,  14.,   4.,\n",
      "        58.,  20.,  39.,  14.,  55.,   2.,   0.,  31.,   0.,  35.,  34.,\n",
      "        15.,  28.,   8.,  38.,   0.,  19.,   0.,   0.,  40.,   0.], dtype=float32), 'SibSp': array([ 1.,  1.,  0.,  1.,  0.,  0.,  0.,  3.,  0.,  1.,  1.,  0.,  0.,\n",
      "        1.,  0.,  0.,  4.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  3.,  1.,\n",
      "        0.,  3.,  0.,  0.,  0.,  1.], dtype=float32), 'Parch': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  0.,  1.,  0.,  0.,\n",
      "        5.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  5.,\n",
      "        0.,  2.,  0.,  0.,  0.,  0.], dtype=float32), 'Fare': array([   7.25      ,   71.28330231,    7.92500019,   53.09999847,\n",
      "          8.05000019,    8.45829964,   51.86249924,   21.07500076,\n",
      "         11.13329983,   30.07080078,   16.70000076,   26.54999924,\n",
      "          8.05000019,   31.27499962,    7.85419989,   16.        ,\n",
      "         29.125     ,   13.        ,   18.        ,    7.2249999 ,\n",
      "         26.        ,   13.        ,    8.0291996 ,   35.5       ,\n",
      "         21.07500076,   31.38750076,    7.2249999 ,  263.        ,\n",
      "          7.87919998,    7.89580011,   27.7208004 ,  146.52079773], dtype=float32), 'Title': array([  6.,   7.,   4.,   7.,   6.,   6.,   6.,   3.,   7.,   7.,   4.,\n",
      "         4.,   6.,   6.,   4.,   7.,   3.,   6.,   7.,   7.,   6.,   6.,\n",
      "         4.,   6.,   4.,   7.,   6.,   6.,   4.,   6.,  10.,   7.], dtype=float32), 'Embarked': array([ 3.,  1.,  3.,  3.,  3.,  2.,  3.,  3.,  3.,  1.,  3.,  3.,  3.,\n",
      "        3.,  3.,  3.,  2.,  3.,  3.,  1.,  3.,  3.,  2.,  3.,  3.,  3.,\n",
      "        1.,  3.,  2.,  3.,  1.,  1.], dtype=float32), 'FamilySizeFamilyID': array([ 2.,  2.,  1.,  2.,  1.,  1.,  1.,  5.,  3.,  2.,  3.,  1.,  1.,\n",
      "        7.,  1.,  1.,  6.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  5.,  7.,\n",
      "        1.,  6.,  1.,  1.,  1.,  2.], dtype=float32)}, array([[0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [0],\n",
      "       [1],\n",
      "       [0],\n",
      "       [1],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [0],\n",
      "       [1],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "next_batch = titanic_input_fn(train_path, False) # Will return first 32 elements\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    first_batch = sess.run(next_batch)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it is working!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now we will define our DNN estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './checkpoints', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f90c284a358>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "%rm -r ./checkpoints\n",
    "\n",
    "# path to save checkpoints\n",
    "save_dir = './checkpoints'\n",
    "\n",
    "# reset default graph if rebuilding the classifier\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create the feature_columns, which specifies the input to our model.\n",
    "# All our input features are numeric, so use numeric_column for each one.\n",
    "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]\n",
    "\n",
    "# define the classifier\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns, # The input features to our model\n",
    "    hidden_units=[2048, 1024, 512, 256, 128], # 5 layers\n",
    "    n_classes=2, # survived or not {1, 0}\n",
    "    model_dir=save_dir, # Path to where checkpoints etc are stored\n",
    "    optimizer=tf.train.RMSPropOptimizer(\n",
    "        learning_rate=0.00001),\n",
    "    dropout=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train the model using `titanic_input_fn` and our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train_split_final.csv'\n",
    "valid_path = './data/valid_split_final.csv'\n",
    "test_path = './data/test_final.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./checkpoints/model.ckpt.\n",
      "INFO:tensorflow:loss = 25.8542, step = 1\n",
      "INFO:tensorflow:global_step/sec: 243.864\n",
      "INFO:tensorflow:loss = 18.1964, step = 101 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.23\n",
      "INFO:tensorflow:loss = 21.0345, step = 201 (0.350 sec)\n",
      "INFO:tensorflow:global_step/sec: 291.351\n",
      "INFO:tensorflow:loss = 18.289, step = 301 (0.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 276.521\n",
      "INFO:tensorflow:loss = 18.0048, step = 401 (0.361 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.284\n",
      "INFO:tensorflow:loss = 16.4129, step = 501 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.576\n",
      "INFO:tensorflow:loss = 18.8377, step = 601 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.436\n",
      "INFO:tensorflow:loss = 21.0404, step = 701 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 269.046\n",
      "INFO:tensorflow:loss = 16.477, step = 801 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.686\n",
      "INFO:tensorflow:loss = 17.1671, step = 901 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.222\n",
      "INFO:tensorflow:loss = 15.9457, step = 1001 (0.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 273.878\n",
      "INFO:tensorflow:loss = 16.4629, step = 1101 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 288.138\n",
      "INFO:tensorflow:loss = 16.8868, step = 1201 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.993\n",
      "INFO:tensorflow:loss = 20.0271, step = 1301 (0.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 276.724\n",
      "INFO:tensorflow:loss = 14.2092, step = 1401 (0.361 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.401\n",
      "INFO:tensorflow:loss = 16.429, step = 1501 (0.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.892\n",
      "INFO:tensorflow:loss = 19.0871, step = 1601 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.582\n",
      "INFO:tensorflow:loss = 16.3273, step = 1701 (0.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 274.789\n",
      "INFO:tensorflow:loss = 22.8615, step = 1801 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.352\n",
      "INFO:tensorflow:loss = 18.7684, step = 1901 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.571\n",
      "INFO:tensorflow:loss = 16.1061, step = 2001 (0.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.94\n",
      "INFO:tensorflow:loss = 15.2128, step = 2101 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.948\n",
      "INFO:tensorflow:loss = 15.0861, step = 2201 (0.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 271.618\n",
      "INFO:tensorflow:loss = 15.4345, step = 2301 (0.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.003\n",
      "INFO:tensorflow:loss = 14.9818, step = 2401 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.076\n",
      "INFO:tensorflow:loss = 20.3488, step = 2501 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.88\n",
      "INFO:tensorflow:loss = 16.843, step = 2601 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.696\n",
      "INFO:tensorflow:loss = 12.5537, step = 2701 (0.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 300.12\n",
      "INFO:tensorflow:loss = 15.8548, step = 2801 (0.334 sec)\n",
      "INFO:tensorflow:global_step/sec: 297.803\n",
      "INFO:tensorflow:loss = 12.7231, step = 2901 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 297.054\n",
      "INFO:tensorflow:loss = 14.9612, step = 3001 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 288.773\n",
      "INFO:tensorflow:loss = 19.0002, step = 3101 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.334\n",
      "INFO:tensorflow:loss = 14.4163, step = 3201 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 278.043\n",
      "INFO:tensorflow:loss = 18.5253, step = 3301 (0.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.748\n",
      "INFO:tensorflow:loss = 22.9252, step = 3401 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 275.129\n",
      "INFO:tensorflow:loss = 15.4716, step = 3501 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.89\n",
      "INFO:tensorflow:loss = 15.3374, step = 3601 (0.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.271\n",
      "INFO:tensorflow:loss = 15.1569, step = 3701 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.282\n",
      "INFO:tensorflow:loss = 14.3849, step = 3801 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.434\n",
      "INFO:tensorflow:loss = 18.174, step = 3901 (0.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.17\n",
      "INFO:tensorflow:loss = 15.7519, step = 4001 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.408\n",
      "INFO:tensorflow:loss = 14.0272, step = 4101 (0.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.667\n",
      "INFO:tensorflow:loss = 14.7298, step = 4201 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.48\n",
      "INFO:tensorflow:loss = 19.6015, step = 4301 (0.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.357\n",
      "INFO:tensorflow:loss = 18.418, step = 4401 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.401\n",
      "INFO:tensorflow:loss = 11.1261, step = 4501 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.941\n",
      "INFO:tensorflow:loss = 12.8699, step = 4601 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 275.387\n",
      "INFO:tensorflow:loss = 20.0675, step = 4701 (0.363 sec)\n",
      "INFO:tensorflow:global_step/sec: 252\n",
      "INFO:tensorflow:loss = 15.6602, step = 4801 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.868\n",
      "INFO:tensorflow:loss = 17.1045, step = 4901 (0.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 273.767\n",
      "INFO:tensorflow:loss = 19.0516, step = 5001 (0.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.03\n",
      "INFO:tensorflow:loss = 13.8127, step = 5101 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 269.626\n",
      "INFO:tensorflow:loss = 13.7659, step = 5201 (0.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 274.548\n",
      "INFO:tensorflow:loss = 15.2307, step = 5301 (0.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.935\n",
      "INFO:tensorflow:loss = 19.1951, step = 5401 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.587\n",
      "INFO:tensorflow:loss = 16.1776, step = 5501 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.992\n",
      "INFO:tensorflow:loss = 16.4523, step = 5601 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.493\n",
      "INFO:tensorflow:loss = 19.8324, step = 5701 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.08\n",
      "INFO:tensorflow:loss = 14.3252, step = 5801 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.761\n",
      "INFO:tensorflow:loss = 19.654, step = 5901 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.304\n",
      "INFO:tensorflow:loss = 15.2844, step = 6001 (0.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 284.455\n",
      "INFO:tensorflow:loss = 12.7026, step = 6101 (0.352 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.446\n",
      "INFO:tensorflow:loss = 15.0085, step = 6201 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.33\n",
      "INFO:tensorflow:loss = 17.4604, step = 6301 (0.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.182\n",
      "INFO:tensorflow:loss = 15.9759, step = 6401 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.013\n",
      "INFO:tensorflow:loss = 14.8385, step = 6501 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.296\n",
      "INFO:tensorflow:loss = 21.7655, step = 6601 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.297\n",
      "INFO:tensorflow:loss = 10.5201, step = 6701 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 270.979\n",
      "INFO:tensorflow:loss = 13.5208, step = 6801 (0.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.826\n",
      "INFO:tensorflow:loss = 15.7064, step = 6901 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.461\n",
      "INFO:tensorflow:loss = 15.748, step = 7001 (0.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.994\n",
      "INFO:tensorflow:loss = 18.6634, step = 7101 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.75\n",
      "INFO:tensorflow:loss = 14.2601, step = 7201 (0.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.503\n",
      "INFO:tensorflow:loss = 19.9954, step = 7301 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.048\n",
      "INFO:tensorflow:loss = 11.9412, step = 7401 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.695\n",
      "INFO:tensorflow:loss = 20.9835, step = 7501 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.429\n",
      "INFO:tensorflow:loss = 14.9772, step = 7601 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 288.887\n",
      "INFO:tensorflow:loss = 16.4065, step = 7701 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 285.382\n",
      "INFO:tensorflow:loss = 10.8126, step = 7801 (0.350 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.999\n",
      "INFO:tensorflow:loss = 13.6248, step = 7901 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.255\n",
      "INFO:tensorflow:loss = 16.401, step = 8001 (0.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.591\n",
      "INFO:tensorflow:loss = 9.38192, step = 8101 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.081\n",
      "INFO:tensorflow:loss = 11.6938, step = 8201 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.426\n",
      "INFO:tensorflow:loss = 14.2205, step = 8301 (0.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 11.7054, step = 8401 (0.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.69\n",
      "INFO:tensorflow:loss = 14.7185, step = 8501 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.14\n",
      "INFO:tensorflow:loss = 19.1348, step = 8601 (0.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.972\n",
      "INFO:tensorflow:loss = 12.1516, step = 8701 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.994\n",
      "INFO:tensorflow:loss = 18.1426, step = 8801 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 273.929\n",
      "INFO:tensorflow:loss = 14.3482, step = 8901 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.85\n",
      "INFO:tensorflow:loss = 20.474, step = 9001 (0.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.907\n",
      "INFO:tensorflow:loss = 16.1795, step = 9101 (0.443 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.706\n",
      "INFO:tensorflow:loss = 12.8834, step = 9201 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.595\n",
      "INFO:tensorflow:loss = 10.8068, step = 9301 (0.350 sec)\n",
      "INFO:tensorflow:global_step/sec: 277.714\n",
      "INFO:tensorflow:loss = 10.7849, step = 9401 (0.361 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.762\n",
      "INFO:tensorflow:loss = 15.6409, step = 9501 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.601\n",
      "INFO:tensorflow:loss = 14.5539, step = 9601 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.095\n",
      "INFO:tensorflow:loss = 16.4794, step = 9701 (0.378 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9750 into ./checkpoints/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 13.0387.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7f90c284a080>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the classifier will run for 500 epochs below\n",
    "classifier.train(input_fn=lambda: titanic_input_fn(train_path, True, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-06-12:35:07\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/model.ckpt-9750\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-06-12:35:07\n",
      "INFO:tensorflow:Saving dict for global step 9750: accuracy = 0.764045, accuracy_baseline = 0.588015, auc = 0.803561, auc_precision_recall = 0.703048, average_loss = 0.551122, global_step = 9750, label/mean = 0.411985, loss = 16.35, prediction/mean = 0.371183\n",
      "\n",
      "Evaluation results:\n",
      "   accuracy, was: 0.7640449404716492\n",
      "   accuracy_baseline, was: 0.5880149602890015\n",
      "   auc, was: 0.8035610914230347\n",
      "   auc_precision_recall, was: 0.703047513961792\n",
      "   average_loss, was: 0.55112224817276\n",
      "   label/mean, was: 0.41198500990867615\n",
      "   loss, was: 16.349960327148438\n",
      "   prediction/mean, was: 0.37118253111839294\n",
      "   global_step, was: 9750\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "# Return value will contain evaluation_metrics such as: loss & average_loss\n",
    "evaluate_result = classifier.evaluate(\n",
    "   input_fn=lambda: titanic_input_fn(valid_path, False, 1))\n",
    "print('')\n",
    "print(\"Evaluation results:\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test file\n",
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/model.ckpt-9750\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1310\n"
     ]
    }
   ],
   "source": [
    "predict_results = classifier.predict(\n",
    "    input_fn=lambda: titanic_input_fn(test_path, False, 1))\n",
    "print(\"Predictions on test file\")\n",
    "i=892\n",
    "for prediction in predict_results:\n",
    "    # Will print the predicted class: 0 or 1.\n",
    "    print(prediction[\"class_ids\"][0])\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
